\chapter{Model Order Reduction} \label{chapter:3}
Mathematical simulation is now evermore important in engineering, science, and related domains, thanks to expeditious advances in computational sciences and rapid growth in computational capacities over the past decades. Numerical evaluation of partial differential equations lies at the core of these disciplines which accommodates design, optimization, and prediction of inputs and outputs of interest. However, the exceeding need for accuracy, and the complexity of multi-physical new applications makes conventional approaches for solving large scale systems of partial differential equations impractical. 

To cope with these limitations, \emph{reduced-order} modeling (ROM), appose to \emph{full-order} or \emph{high-fidelity} modelling, has been an area of active research for the past decade. These methods eliminate the redundant physical or computation dimensions of the full-order differential equation to construct a low dimensional reduced-order system. This approximation in return significantly accelerates the evaluation of the system. Reduced basis (RB) methods, are among the most successful ROMs and is vastly used in academia and industry. RB methods, seek a low dimensional reduced subspace that accurately represents the full-order solution. Confining the system on this subspace, using a projection, can then accelerate the evaluation of the system. These methods are particularly successful in reducing the computational complexity of \emph{parametric PDEs} or multi-query systems, where a system of differential equations needs to be evaluated for a large number of input parameters.

In this chapter we summarize the fundamentals of MOR and especially RB methods. We present various conventional approaches and algorithms for linear and nonlinear problems. Since time, as a parameter, is particularly important in the context of Hamiltonian systems, we will develop this chapter with an emphasis on time-dependent problems.

\section{Solution Manifold and Reduced Basis Methods} \label{sec:3.1}
In this chapter we consider parametric dynamical systems of the type
\begin{equation} \label{eq:3.1}
\left\{
\begin{aligned}
	\frac d{dt} u(t;\mu) &= f(t,u;\mu),\\
	u(0;\mu) &= u_0(\mu).
\end{aligned}
\right.
\end{equation}
Here $u,u_0\in \mathbb R^{n}$, $f:\mathbb R \times \mathbb R^{n} \times \mathbb P\to \mathbb R^{n}$ is a smooth linear or nonlinear function, and $\mu \in \mathbb P$, where $\mathbb P$ is a compact subset of $\mathbb R^r$. It is well known that for a fixed $\mu$, \cref{eq:3.1} has a unique solution if $f$ is continuous with a continuous derivative \cite{rudin1976principles}. Note that for parametric PDEs, we may use the method of lines \cite{edsberg2015introduction} to obtain a dynamical system of the form (\ref{eq:3.1}).

To obtain a numerical evaluation of the solution to \cref{eq:3.1} for a fixed $\mu$, we may use some time integration method, e.g. the Runge-Kutta methods \cite{edsberg2015introduction}. This provides us with an approximate solution $\tilde u(t_i) \approx u(t_i)$ for time instances $i = 1 , \dots , N_t$. Throughout this chapter we assume that the $\tilde u$ can be chosen arbitrary close to $u$, and by abusing the notation, we may omit the ``tilde'' sign. In the MOR community, $u$ is often referred to as the \emph{full-order} or the \emph{high-fidelity} solution \cite{hesthaven2015certified,quarteroni2015reduced}. 

\begin{definition}
The solution manifold is a set of all solutions to \cref{eq:3.1} under the variation of the parameter vector $\mu$, i.e.
\begin{equation} \label{eq:3.2}
	\mathcal M_{u} = \{ u(t;\mu) | t \in [0,T] , \mu \in \mathbb P \}.
\end{equation}
\end{definition}
Note that the solution manifold is not always smooth. A main assumption in an RB method is that $\mathcal M_{u}$ has a low dimensional representation. This allows us to chose a few number of basis vectors $W = \{ w_1,\dots,w_k\}$, with $k\ll n$, where $\mathcal W = \text{span}(W)$ represents $\mathcal M_{u}$ with a small error. $W$ is often referred to as the \emph{reduced basis}. To understand when does a low dimensional reduced basis exist and to quantify the error incur in approximation, we need to introduce the notion of the \emph{Kolmogorov $n$-width} \cite{pinkus1985n}.

\begin{definition}
Let $\mathcal W$ be a subset of a Banach space $\mathcal X$. The distance of a point $x\in \mathcal X$ form $\mathcal Y$ is given by
\begin{equation}  \label{eq:3.3}
	\text{dist}(x,\mathcal W) := \inf_{w\in \mathcal W} \| x-w \|.
\end{equation}
where $\|\cdot\|$ is the norm defined on $\mathcal X$.
\end{definition}
We can also look at $\text{dist}(x,\mathcal W)$ as a measure on how well we can approximate $x$ with elements in $\mathcal W$.
\begin{definition}
Let $\mathcal S$ be a compact subset of a Banach space $\mathcal X$. The Kolmogorov $n$-width of $S$ is defined as
\begin{equation} \label{eq:3.4}
	d_n(\mathcal S) = \inf_{\mathcal W_n} \ \sup_{s\in \mathcal S} \ \text{dist}(s, \mathcal W_n),
\end{equation}
where the infimum is carried over all possible linear subspaces $\mathcal W_n$ of dimension $n$.
\end{definition}
Therefore, the $n$-width measures how well $\mathcal S$ can be approximated by a subspace of dimension $n$. Note that when $\mathcal X$ is also equipped with an inner product operator $<\cdot , \cdot> :\mathcal X \times \mathcal X \to \mathbb R $, such that $\| \cdot \| = \sqrt{<\cdot , \cdot>}$, then $\text{dist}(x,\mathcal W) = \| x - P_{<,>,\mathcal W}(x) \|$, where $P_{<,>,\mathcal W}$ is the projection operator with respect to $<\cdot,\cdot>$ onto $\mathcal W$. In this case $\text{dist}(x,\mathcal W)$ is often referred to as the \emph{projection error}.

As in an RB method tends to approximate $\mathcal M_u$ with a low dimensional subspace $\mathcal W$, it is natural to use the $n$-width terminology. To achieve an accurate RB approximation we would like to truncate the sequence $d_1(\mathcal M_u),d_2(\mathcal M_u),\dots, d_n(\mathcal M_u)$ such that the truncation error is negligible, i.e.
\begin{equation} \label{eq:3.5}
	\frac{\sum_{i=1}^k d_i(\mathcal M_u) }{\sum_{i=1}^n d_i(\mathcal M_u) } < \delta,
\end{equation}
for some small tolerance $\delta$. Therefore, it is desirable that the above sequence has an exponential decay, in which case $\mathcal M_u$ is referred to as \emph{reducible}. Note that in general, the dimension $k$ must be chosen small enough to guarantee computational gain. Once the subspace $\mathcal W_k$ is chosen we can construct the projection operator $P_{<,>,\mathcal W_k}$ to write the reduced-order system
\begin{equation} \label{eq:3.6}
\left\{
\begin{aligned}
	\frac d{dt} P_{<,>,\mathcal W_k}(u(t;\mu)) &= P_{<,>,\mathcal W_k}(f(t,u;\mu)),\\
	P_{<,>,\mathcal W_k}(u(0;\mu)) &= P_{<,>,\mathcal W_k}( u_0(\mu) ).
\end{aligned}
\right.
\end{equation}
Note that in this thesis, we assume that the projection operator $P_{<,>,\mathcal W_k}$ (and subsequently the reduced space $\mathcal W_k$) is not time dependent\footnote{We refer the reader to \cite{doi:10.1137/140967787,doi:10.1137/16M1095202} for dynamically orthogonal reduced basis method: an RB method with a time varying basis.}. Therefore, we may commute the projection operator with the time derivation operator. Given that $k \ll n$, then potentially \cref{eq:3.6} can be evaluated with a lower computational complexity compared to \cref{eq:3.1}. However, reducibility of $\mathcal M_u$ does not say anything about the stability of \cref{eq:3.6}. As the matter of fact, \cref{eq:3.6} could be unstable even if \cref{eq:3.1} is a stable dynamical system \cite{doi:10.1137/140978922,doi:10.1137/17M1111991}. In \Cref{chapter:4,chapter:5,chapter:6} we discuss how we can enhance the stability of \cref{eq:3.6} given that \cref{eq:3.1} is a Hamiltonian system.

In the following sections we summarize numerical methods for choosing the proper dimension $k$, finding the reduced space $\mathcal W_k$, constructing the projection operator $P_{<,>,\mathcal W_k}$ and also efficient ways to construct and integrate the reduced system (\ref{eq:3.6}).

\section{Proper Orthogonal Decomposition} \label{sec:3.2}
to numerically evaluate $\mathcal Y_k$, we first discretize the solution manifold $\mathcal M_{u}$ into a point cloud $\mathcal M_{u}^{\Delta}$ defined as
\begin{equation} \label{eq:3.7}
	\mathcal M_{u}^{\Delta} : = \{ u(t_i;\mu) |  1\leq i \leq N_t \text{ and } \mu \in \mathbb P^{\Delta} \},
\end{equation}
where $\mathbb P^{\Delta} = \{ \mu_{1} , \dots , \mu_{N_{\mathbb P}} \}$ is a finite set representing $\mathbb P$. Note that the choice of $\mathbb P^{\Delta}$ is generally not trivial and is often problem dependent. We refer the reader to \cite{quarteroni2015reduced} for more information on discretizing the parameter space.

Each member of $\mathcal M_{u}^{\Delta}$ is a vector in $\mathbb R^{n}$ and is commonly referred to a \emph{snapshot}. Suppose that we can find $k$ basis vectors $w_1,\dots,w_k\in \mathbb R^{n}$, orthonormal with respect to some inner product operator $<\cdot,\cdot>$, and with a span space $\mathcal W$ which approximately represent the span$(\mathcal M_u^\Delta)$. As discussed in \Cref{sec:3.1}, the projection error of approximating a member of $\mathcal M_{u}^{\Delta}$ with an element of $\mathcal Y_k$ is given by
\begin{equation} \label{eq:3.8}
	e_{\mathcal W_k}(s) = \| s - P_{<,>,\mathcal W_k}(s) \|, \quad s\in \mathcal M_u^{\Delta},
\end{equation}
where $\| \cdot \|$ is the norm associated with $<\cdot,\cdot>$ and $P_{<,>,\mathcal W_k}$ is the orthogonal projection operator given by
\begin{equation} \label{eq:3.9}
	P_{<,>,\mathcal W_k}(s) = \sum_{i=1}^k <s,w_i> w_i.
\end{equation}
The \emph{proper orthogonal decomposition} method is then finding the $\mathcal W_k$ (for a fixed $k$) that minimizes the collective projection error, and corresponds to the minimization problem
\begin{equation} \label{eq:3.10}
\begin{aligned}
&  \underset{\mathcal W_k}{\text{minimize}}
& & \sum_{s\in \mathcal M_u^{\Delta}} \| s - P_{<,>,\mathcal W_k} (s)\|^2, \\
& \text{subject to}
& & <w_i,w_j> = \delta_{i,j}, \quad 1\leq i,j \leq k.
\end{aligned}
\end{equation}
This formulation is comparable with the discrete version of the Kolmogorov $k$-width. 
\subsection{Euclidean Inner Product}When $<\cdot,\cdot>$ is the classical Euclidean inner product, i.e. $<a,b> = a^Tb$ for $a,b\in \mathbb R^{n}$, then we can rewrite the projection operator in (\ref{eq:3.9}) as
\begin{equation}
	P_{I,\mathcal W_k} (s) = WW^T s.
\end{equation}
Here $W = [w_i]_{i=1}^k$ is the matrix containing the basis vectors of $\mathcal W_k$ (Note that we used the super script $I$ to indicate the Euclidean inner product in $P_{I,\mathcal W_k}$. To avoid confusion, we may also use this super script also for $<\cdot,\cdot>_I$). Furthermore, the constraints in minimization (\ref{eq:3.10}) simplify to $W^TW = I_k$. Thus, (\ref{eq:3.10}) becomes
\begin{equation} \label{eq:3.11}
\begin{aligned}
&  \underset{W\in\mathbb R^{n\times k}}{\text{minimize}}
& & \sum_{s\in \mathcal M_u^{\Delta}} \| s - WW^Ts\|^2_2, \\
& \text{subject to}
& & W^TW=I_k.
\end{aligned}
\end{equation}
Here $\|\cdot \|_2$ is the Euclidean 2-norm. Finally, if we collect all the snapshots into the \emph{snapshot matrix}
\begin{equation} \label{eq:3.12}
	S = [u(t_i;\mu_j)],\quad 1\leq i \leq N_t,\quad 1 \leq j \leq N_{\mathbb P},
\end{equation}
we can then use basic results in linear algebra \cite{trefethen97} to reformulate (\ref{eq:3.10}) as
\begin{equation} \label{eq:3.13}
\begin{aligned}
&  \underset{W\in\mathbb R^{n\times k}}{\text{minimize}}
& & \| S - WW^TS \|_F, \\
& \text{subject to}
& & W^TW=I_k.
\end{aligned}
\end{equation}
This minimization is nonlinear and non-convex in general. However, one of the most remarkable results in numerical analysis relates the solution to this minimization with an eigenvalue problem on $S$. We summarize this in the following theorem and refer the reader to \cite{Markovsky:2011:LRA:2103589} for the proof.
\begin{theorem} \label{theorem:3.1}
(Eckart-Young-Mirsky-Schmidt) Suppose that $D\in \mathbb R^{m\times n}$ ($m<n$) has the singular value decomposition (SVD) \cite{Markovsky:2011:LRA:2103589}, $D = U \Sigma V^T$. Consider the partitioning for $U$, $\Sigma$ and $V$ as
\begin{equation} \label{eq:3.14}
	U = [U_1 U_2], \quad \Sigma =
	\begin{bmatrix}
		\Sigma_1 & 0 \\
		0 & \Sigma_2
	\end{bmatrix}
	, \quad V = [V_1 V_2],
\end{equation}
where $U_1\in \mathbb R^{n\times r}$, $U_2 \in \mathbb R^{n\times (n-r)}$, $\Sigma_1 \in \mathbb R^{r\times r}$, $\Sigma_2 \in \mathbb R^{(n-r)\times (n-r)}$, $V_1 \in \mathbb R^{n\times r}$ and $V_2 \in \mathbb R^{n\times (n-r)}$. Then the rank $r$ matrix resulted from the truncation of the SVD decomposition
\begin{equation} \label{eq:3.15}
	\tilde D = U_1 \Sigma_1 V_1^T,
\end{equation}
minimizes the minimization problem
\begin{equation} \label{eq:3.16}
\begin{aligned}
&  \underset{M\in\mathbb R^{m\times n}}{\text{minimize}}
& & \| D - M \|_F, \\
& \text{subject to}
& & \text{rank}(M) = r.
\end{aligned}
\end{equation}
Furthermore, $\| D - \tilde D \|_F = \sum_{i=r+1}^{m} \sigma_i$, where $\sigma_i$ is the $i$-th singular value of $D$.
\end{theorem} \label{theorem:3.2}
With this, we immediately find the solution to (\ref{eq:3.13}).
\begin{theorem} \label{theorem:3.2}
\cite{doi:10.1137/1.9780898718713} Let $S = U \Sigma V^T$ be the SVD decomposition of $S$ with $U = [u_i]_{i=1}^n$. Then $W = U_1$ is the solution to the minimization (\ref{eq:3.13}) where $\tilde S = U_1\Sigma_1 V_1^T$ is the rank $k$ approximation of $S$ in \Cref{theorem:3.1}.
\end{theorem}

\begin{proof}
Let $\tilde S = U_1 \Sigma_1 V_1^T$ be the matrix that minimizes $\| S - \tilde S \|_F$ in \Cref{theorem:3.1} and let $\tilde \Sigma$ be defined as
\begin{equation} \label{eq:3.17}
	\tilde \Sigma =
	\begin{bmatrix}
		\Sigma_1 & 0 \\
		0 & 0
	\end{bmatrix}.	
\end{equation}
It is easily verified that $\tilde S = U_1 \Sigma_1 V_1^T = U \tilde \Sigma V^T$. It yields
\begin{equation} \label{eq:3.18}
	\tilde S =  U \tilde \Sigma V^T = U \tilde \Sigma \Sigma^{-1} U^T S = U_1 U_1^T S.
\end{equation}
Therefore, minimizing $\| S - \tilde S \|_F$ for a rank $k$ matrix $\tilde S$ is equivalent to minimizing $\| S - U_1U_1^T S \|_F$ for all $U_1$ such that $U_1^T U_1 = I_k$.
\end{proof}
As the minimization (\ref{eq:3.13}) is closely related to the Kolmogorov $k$-width of $\mathcal M_u$, \Cref{theorem:3.1} suggests that the decay of the singular values of $S$ is an indicator for the decay of the Kolmogorov $n$-width of $\mathcal M_u$. This is a numerical approach to understand reducibility of $\mathcal M_u$. \Cref{alg:3.1} summarizes POD for generating an orthonormal basis with respect to the Euclidean inner product.

\begin{algorithm} 
	\caption{POD for constructing an orthonormal reduced basis with respect to the Euclidean inner product} \label{alg:3.1}
	\textbf{Input:} snapshot matrix $S$, tolerance value $\delta$.
	\begin{algorithmic} [1]
		\State compute the SVD decomposition $S = U \Sigma V^T$.
		\State pick $k$ the largest number such that that
		\[
			\frac{\sum_{i=k+1}^n \sigma_i}{\sum_{i=1}^n \sigma_i} < \delta.
		\]
		\State define $W = [u_i]_{i=1}^k$.
	\end{algorithmic}
	\vspace{0.5cm}
	\textbf{Output:} reduced basis $W$.
\end{algorithm}

\subsection{Non-Euclidean Inner Product}
Now suppose that $<,>$ is a non-Euclidean inner product. One can associate such an inner product with a symmetric and positive definite matrix $X$ (and denote $<,>_X$) such that $<a,b>_X = a^TXb$ for all $a,b\in \mathbb R^n$. Given an orthonormal basis $W = [w_i]_{i=1}^k$ with respect to this inner product, it is easy to verify that the orthogonal projection onto $\mathcal W_k = \text{span}(W)$ is given by
\begin{equation} \label{eq:3.19}
	P_{X,\mathcal W_k}(s) = WW^TXs, \quad s\in \mathbb R^{n}.
\end{equation}
With this, the minimization (\ref{eq:3.10}) becomes
\begin{equation} \label{eq:3.20}
\begin{aligned}
&  \underset{W\in\mathbb R^{n\times k}}{\text{minimize}}
& & \sum_{s\in \mathcal M_u^{\Delta}} \| s - WW^TXs\|^2_X, \\
& \text{subject to}
& & W^TXW=I_k.
\end{aligned}
\end{equation}
Here $\| \cdot \|_X = \sqrt{<\cdot , \cdot>_X}$ and the constraint is due to the fact that $<w_i,w_j>_X = w_i^TXw_j = \delta_{ij}$ for $i=1,\dots,k$. It follows that
\begin{equation} \label{eq:3.21}
\begin{aligned}
	\sum_{s\in \mathcal M_u^{\Delta}} \| s - WW^TXs\|^2_X &= \sum_{s\in \mathcal M_u^{\Delta}} \| X^{1/2}s - X^{1/2}WW^TXs\|_2^2 \\
	&= \sum_{s\in \mathcal M_u^{\Delta}} \| X^{1/2}s - X^{1/2}WW^TX^{1/2} X^{1/2}s\|_2^2 \\
	& = \| X^{1/2}S - X^{1/2}WW^TX^{1/2} X^{1/2}S \|_F^2.
\end{aligned}
\end{equation}
Here $X^{1/2}$ is the matrix square root of $X$. Now if we define $\bar S = X^{1/2} S$ and $\bar W = X^{1/2} W$, we can rewrite (\ref{eq:3.20}) as 
\begin{equation} \label{eq:3.22}
\begin{aligned}
&  \underset{\bar W\in\mathbb R^{n\times k}}{\text{minimize}}
& & \| \bar S - \bar W \bar W^T \bar S\|_F, \\
& \text{subject to}
& & \bar W^T\bar W=I_k.
\end{aligned}
\end{equation}
According to \Cref{theorem:3.1,theorem:3.2} the solution $\bar W$ to this minimization is the rank $k$ approximation of $\bar S$. We can then compute $W$ form $W = X^{-1/2}\bar W$. Constructing a POD basis with respect to a non-Euclidean inner product is presented in \Cref{alg:3.2}.

\begin{algorithm} 
	\caption{POD for constructing an orthonormal reduced basis with respect to a non-Euclidean inner product} \label{alg:3.2}
	\textbf{Input:} snapshot matrix $S$, weight matrix $X$, and tolerance value $\delta$.
	\begin{algorithmic} [1]
		\State compute $\bar S = X^{1/2} S$.
		\State compute the SVD decomposition $\bar S = U \Sigma V^T$.
		\State pick $k$ the largest number such that that
		\[
			\frac{\sum_{i=k+1}^n \sigma_i}{\sum_{i=1}^n \sigma_i} < \delta.
		\]
		\State define $\bar W = [u_i]_{i=1}^k$.
		\State compute $W = X^{-1/2} \bar W$.
	\end{algorithmic}
	\vspace{0.5cm}
	\textbf{Output:} reduced basis $W$.
\end{algorithm}
For large scale problems, computing the square root of $X$ could be computationally demanding. Let $\bar S = U\Sigma V$ be the SVD decomposition of $\bar S$ with $\{u_i\}_{i=1}^{n}$, $\{v_i\}_{i=1}^{n}$, and $\{ \sigma_i \}_{i=1}^n$ the left singular vectors, right singular vectors and singular values, respectively. We have
\begin{equation} \label{eq:3.23}
	S^TXS v_i = \bar S^T \bar S v_i = \sigma_i \bar S^T v_i^T = \sigma_i^2 u_i. 
\end{equation}
Here we used properties of an SVD decomposition \cite{trefethen97}. This suggests that $\{\sigma_i^2\}_{i=1}^{n}$ and $\{v_i\}_{i=1}^n$ are the eigenvalues ands eigenvectors of $S^TXS$, respectively. Matrix $G:=S^TXS$ is commonly referred to as the \emph{Gramian matrix}. To obtain the POD basis we can then write
\begin{equation} \label{eq:3.24}
	w_i = X^{-1/2} u_i = \sigma_i^{-1} X^{-1/2} \bar S v_i = \sigma_i^{-1} S v_i.
\end{equation}
Thus, the computation of $X^{1/2}$ can be avoided. We summarize the computationally efficient way to find a POD basis with respect to a non-Euclidean inner product in \Cref{alg:3.3}.

\begin{algorithm} 
	\caption{POD for constructing an orthonormal reduced basis with respect to a non-Euclidean inner product} \label{alg:3.3}
	\textbf{Input:} snapshot matrix $S$, weight matrix $X$.
	\begin{algorithmic} [1]
		\State compute the Gramian matrix $ G = S^TXS$.
		\State solve the eigenvalue problem $Gv_i = \sigma_i^2 v_i$. 
		\State compute $w_i = \sigma_i^{-1} S v_i$.
		\State define basis $W = [w_i]_{i=1}^k$.
	\end{algorithmic}
	\vspace{0.5cm}
	\textbf{Output:} reduced basis $W$.
\end{algorithm}

\section{The Greedy Basis Generation} \label{sec:3.3}

\section{The Galerkin and the Petrov-Galerkin Projection} In section \Cref{sec:3.2,sec:3.3} we discussed computational methods to construct a reduced basis $W$, and subsequently, a reduced space $\mathcal W_k$. In this section we elaborate on how to use a reduced basis to construct the reduced system (\ref{eq:3.6}).

Let $W$ be an orthonormal basis for a reduced space $\mathcal W_k$. We assume $u(t;\mu)$, solution to \Cref{eq:3.1}, can be well approximate in this basis as $u \approx \tilde u = W v$, where $v\in \mathbb R^{k}$ is the expansion coefficients of $\tilde u$ in the basis $W$. Substituting this in 
\Cref{eq:3.1} results in
\begin{equation} \label{eq:3.25}
	W \frac{d}{dt} v(t;\mu) = f(t,Wv;\mu) + r(t,u;\mu).
\end{equation}
Now we can use the properties of the projection operator $P_{I,\mathcal W_k} = WW^T$ to eliminate $W$ from the left hand side. Assuming $r$ is orthogonal to $W$ yields,
\begin{equation} \label{eq:3.26}
	\left\{
	\begin{aligned}
	\frac{d}{dt} v(t;\mu) &= W^T f(t,Wv;\mu), \\
	v(0;\mu) &= W^T u_0(\mu).
	\end{aligned}
	\right.
\end{equation}
Here we used the fact that $W^TW = I_k$. Furthermore, if we instead use the projection $P_{X,\mathcal W} = WW^TX$ we obtain the reduced system
\begin{equation} \label{eq:3.27}
	\left\{
	\begin{aligned}
	\frac{d}{dt} v(t;\mu) &= W^T X f(t,Wv;\mu), \\
	v(0;\mu) &= W^T X u_0(\mu).
	\end{aligned}
	\right.
\end{equation}
Orthogonal projection operators, such as $P_{I,\mathcal W_k}$ and $P_{X,\mathcal W}$ are called a \emph{Galerkin} projection. However, one can construct a non-orthogonal projection $\Pi = WU^T$ by finding $U\in \mathbb R^{n\times k}$ such that $U^TW = I_k$ \cite{hesthaven2015certified,quarteroni2015reduced,doi:10.1137/1.9780898718713}. Such projection is called a \emph{Petrov-Galerkin} projection. Assuming $r$, in \Cref{eq:3.25}, is orthogonal to $U$, $\Pi$ results the reduced system
\begin{equation} \label{eq:3.28}
	\left\{
	\begin{aligned}
	\frac{d}{dt} v(t;\mu) &= U^T f(t,Wv;\mu), \\
	v(0;\mu) &= U^T u_0(\mu).
	\end{aligned}
	\right.
\end{equation}
Note that for the purpose the of most efficient computation, it is important to identify time intensive high dimensional quantities and the cheaper low dimensional computations. This segregation or compartmentalization of quantities, according to their computational cost, is referred to as the \emph{offline/online} decomposition \cite{quarteroni2015reduced}. We tolerate some amount of computational complexity in the offline phase to achieve substantial computation acceleration in the online phase. In the context of RB methods, we tend restrict computations regarding the high-fidelity solution, only to the offline phase. Then we can enjoy fast computations by restricting all computations to the reduced space during the online phase.

Although reduced systems in \Cref{eq:3.26,eq:3.27,eq:3.28} are of a low order, however evaluation of $f(t,Wv;\mu)$ should be performed in the high-fidelity space, for a general $f$. In the following section we discuss how we can avoid this bottleneck. 

\section{Efficient Evaluation of Non-Linear Terms}

In this section we discuss the efficiency of evaluating nonlinear terms in the context of projection based reduced models. Suppose that the right hand side in (\ref{eq:MoOr:1}) is of the form $\mathbf f(t,\mathbf x , \omega) = L\mathbf x + \mathbf g(t,\mathbf x ,\omega)$, where $L\in \mathbb R^{n\times n}$ reflects the linear part, and $\mathbf g$ is a nonlinear function. Now assume that a $k$-dimensional reduced basis $V$ is provided. The reduced system takes the form
\begin{equation} \label{eq:MoOr:8}
	\frac{d}{dt} \mathbf y = \underbrace{(W^TV)^{-1} L V}_{\tilde L} \mathbf{y} + \underbrace{(W^TV)^{-1} \mathbf g(t,V\mathbf y,\omega)}_{\tilde N (\mathbf y)}.
\end{equation}
Here, $\tilde L$ is a $k\times k$ matrix which can be computed before time integration of the reduced system. However, the evaluation of $\tilde N (\mathbf y)$ has a complexity that depends on $n$, the size of the original system. Suppose that the evaluation of $\mathbf g$ with $n$ components has the complexity $\alpha(n)$, for some function $\alpha$. Then the complexity of evaluating $\tilde N(\mathbf y)$ is $\mathcal{O}(\alpha(n) + 4nk)$ which consists of 2 matrix-vector operations and the evaluation of the nonlinear function, i.e. the evaluation of the nonlinear terms can be as expensive as solving the original system.

To overcome this bottleneck we take an approach similar to that of Section \ref{chap:MoOr.PrOr:1} \cite{Chaturantabut:2010cz,Barrault:2004kz}. Assume that the manifold $\mathcal M_{\mathbf g} = \{ \mathbf g(t,\mathbf x , \omega)| t\in \mathbb R, \mathbf x \in \mathbb R , \omega \in \Gamma\}$ is of a low dimension and that $\mathbf g$ can be approximated by a linear subspace of dimension $m\ll n$, spanned by the basis $\{ u_1 , \dots , u_m \}$, i.e.
\begin{equation} \label{eq:MoOr:10}
	\mathbf g(t,\mathbf x,\omega) \approx U \mathbf c(t,\mathbf x,\omega).
\end{equation}
Here $U$ contains basis vectors $u_i$ and $\mathbf c$ is the vector of coefficients. Now suppose $p_1,\dots,p_m$ are $m$ indices from $\{1,\dots,n\}$ and define an $n\times m$ matrix
\begin{equation} \label{eq:MoOr:11}
	P = [e_{p_1},\dots,e_{p_m}],
\end{equation}
where $e_{p_i}$ is the $p_i$-th column of the identity matrix $I_n$. Multiplying $P$ with $\mathbf g$ selects components $p_1,\dots,p_m$ of $\mathbf g$. If we assume that $P^TU$ is non-singular, the coefficient vector $\mathbf c$ can be uniquely determined from
\begin{equation} \label{eq:MoOr:12}
	P^T \mathbf g = (P^TU)\mathbf c.
\end{equation}
Finally the approximation of $\mathbf g$ is determined by
\begin{equation} \label{eq:MoOr:13}
	\mathbf g(t,\mathbf x,\omega) \approx U \mathbf c(t,\mathbf x,\omega) = U (P^TU)^{-1} P^T \mathbf g(t,\mathbf x,\omega),
\end{equation}
which is referred to as the \emph{Discrete Empirical Interpolation} (DEIM) approximation \cite{Chaturantabut:2010cz}. Applying DEIM to the reduced system (\ref{eq:MoOr:5}) yields
\begin{equation} \label{eq:MoOr:14}
	\frac{d}{dt} \mathbf y = \tilde L \mathbf y + (W^TV)^{-1} U(P^TU)^{-1}P^T \mathbf g(t,V\mathbf y , \omega).
\end{equation}
Note that the matrix $(WV)^{-1} U(P^TU)^{-1}$ can be computed offline and since $\mathbf g$ is evaluated only at $m$ of its components, the evaluation of the nonlinear term in (\ref{eq:MoOr:14}) does not depend on $n$.

To obtain the projection basis $U$, the POD can be applied to the ensemble of samples of the nonlinear term $\mathbf g(t_i,\mathbf x, \omega_j)$ with $i=1,\dots,m$ and $j=1,\dots,n$. There is no additional cost associated with computing the nonlinear snapshots, since they are generated when computing the trajectory snapshot matrix $S$. The interpolating indices $p_1,\dots,p_m$ can be constructed as follows. Given the projection basis $U = \{u_1,\dots,u_m\}$, the first interpolation index $p_1$ is chosen according to the component of $u_1$ with the largest magnitude. The rest of the interpolation indices, $p_2,\dots,p_m$ correspond to the component of the largest magnitude of the residual vector $\mathbf r = u_l - U \mathbf c$. It is shown in \cite{Chaturantabut:2010cz} that if the residual vector is a nonzero vector in each iteration then $P^TU$ is non-singular and (\ref{eq:MoOr:13}) is well defined. 

\begin{algorithm} 
\caption{Discrete Empirical Interpolation Method}
{\bf Input:}  Basis vectors $\{u_1,\dots , u_m\}\subset \mathbb R^n$
\begin{algorithmic} [1]
\State pick $p_1$ to be the index of the largest component of $u_1$.
\State $U \leftarrow [u_1]$
\State $P \leftarrow [p_1]$
\State \textbf{for} $i\leftarrow 2$ \textbf{to} $m$
\State \hspace{0.5cm} solve $(P^TU)\mathbf c = P^T u_i$ for $\mathbf c$
\State \hspace{0.5cm} $\mathbf r \leftarrow u_i - U\mathbf c$
\State \hspace{0.5cm} pick $p_i$ to be the index of the largest component of $\mathbf r$
\State \hspace{0.5cm} $U \leftarrow [u_1,\dots,u_i]$
\State \hspace{0.5cm} $P \leftarrow [p_1,\dots,p_i]$
\State \textbf{end for}
\end{algorithmic}
\vspace{0.5cm}
{\bf Output:} Interpolating indices $\{p_1,\dots,p_m\}$
\end{algorithm}


The numerical solution of (\ref{eq:MoOr:8}) may involve the computation of the Jacobian of the nonlinear function $\mathbf g(t,\mathbf x, \omega)$ with respect to the reduced state variable $\mathbf y$
\begin{equation} \label{eq:MoOr:9}
	\mathbf J_{\mathbf y}(\mathbf g) = (W^TV)^{-1} \mathbf J_{\mathbf x}(\mathbf g) V,
\end{equation}
where $\mathbf J_\alpha(\mathbf g)$ is the Jacobian matrix of $\mathbf g$ with respect to the variable $\alpha$. The complexity of (\ref{eq:MoOr:9}) is $\mathcal{O}(\alpha(n) +2n^2k+2nk^2+2nk)$, comprising several matrix-vector multiplications and an evaluation of the Jacobian which depends on the size of the original system. Approximating the Jacobian in (\ref{eq:MoOr:9}) is usually both problem and discretization dependent. Often the nonlinear function $\mathbf g$ is evaluated component-wise i.e.
\begin{equation} \label{eq:MoOr:15}
	\mathbf g(\mathbf x) =
	\begin{pmatrix}
		g_1(x_1,\dots,x_n) \\
		g_2(x_1,\dots,x_n) \\
		\vdots \\
		g_n(x_1,\dots,x_n)
	\end{pmatrix}
	=
	\begin{pmatrix}
		g_1(x_1) \\
		g_2(x_2) \\
		\vdots \\
		g_n(x_n)
	\end{pmatrix}.
\end{equation}
In such cases the interpolating index matrix $P$ and the nonlinear function $\mathbf g$ commute, i.e.,
\begin{equation} \label{eq:MoOr:16}
	\tilde N(\mathbf y) \approx (W^TV)^{-1} U(P^TU)^{-1}P^T \mathbf g(V\mathbf y) = (W^TV)^{-1} U(P^TU)^{-1}\mathbf g(P^TV\mathbf y)
\end{equation}
If we now take the Jacobian of the approximate function we recover
\begin{equation}
	\mathbf J_{\mathbf y}(\mathbf g) = \underbrace{ (W^TV)^{-1} U(P^TU)^{-1} }_{k\times m} \underbrace{ \mathbf J_{\mathbf x}(\mathbf g(P^T V \mathbf y) ) }_{m\times m} \underbrace{P^T V}_{m\times k}.
\end{equation}
The matrix $(WV)^{-1} U(P^TU)^{-1}$ can be computed offline and the Jacobian is evaluated only for $m\times m$ components. Hence the overall complexity of computing the Jacobian is now independent of $n$. We refer the reader to \cite{Barrault:2004kz,Chaturantabut:2010cz} for more detail.
